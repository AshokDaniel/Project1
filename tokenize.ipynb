{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import nltk.corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abc', 'abc.zip', 'alpino', 'alpino.zip', 'biocreative_ppi', 'biocreative_ppi.zip', 'brown', 'brown.zip', 'brown_tei', 'brown_tei.zip', 'cess_cat.zip', 'gutenberg', 'gutenberg.zip', 'stopwords', 'stopwords.zip', 'wordnet', 'wordnet.zip']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(nltk.data.find(\"corpora\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to C:\\Users\\MURAHARI\n",
      "[nltk_data]     ASHOK\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('gutenberg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "par=\"Hi I am Ashok, Jesus is my Saviour\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent=nltk.sent_tokenize(par)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "words=nltk.word_tokenize(par)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi', 'I', 'am', 'Ashok', ',', 'Jesus', 'is', 'my', 'Saviour']\n"
     ]
    }
   ],
   "source": [
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "bible=nltk.corpus.gutenberg.words('bible-kjv.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[', 'The', 'King', 'James', 'Bible', ']', 'The', ...]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ The King James Bible ] The Old Testament of the King James Bible The First Book of Moses : Called Genesis 1 : 1 In the beginning God created the heaven and the earth . 1 : 2 And the earth was without form , and void ; and darkness was upon the face of the deep . And the Spirit of God moved upon the face of the waters . 1 : 3 And God said , Let there be light : and there was light . 1 : 4 And God saw the light , that it was good : and God divided the light from the darkness . 1 : 5 And God called the light Day , and the darkness he called Night . And the evening and the morning were the first day . 1 : 6 And God said , Let there be a firmament in the midst of the waters , and let it divide the waters from the waters . 1 : 7 And God made the firmament , and divided the waters which were under the firmament from the waters which were above the firmament : and it was so . 1 : 8 And God called the firmament Heaven . And the evening and the morning were the second day . 1 : 9 And God said , Let the waters under the heaven be gathered together unto one place , and let the dry land appear : and it was so . 1 : 10 And God called the dry land Earth ; and the gathering together of the waters called he Seas : and God saw that it was good . 1 : 11 And God said , Let the earth bring forth grass , the herb yielding seed , and the fruit tree yielding fruit after his kind , whose seed is in itself , upon the earth : and it was so . 1 : 12 And the earth brought forth grass , and herb yielding seed after his kind , and the tree yielding fruit , whose seed was in itself , after his kind : and God saw that it was good . 1 : 13 And the evening and the morning were the third day . 1 : 14 And God said , Let there be lights in the firmament of the heaven to divide the day from the night ; and let them be for signs , and for seasons , and for days , and years : 1 : 15 And let them be for lights in the firmament of the heaven to give light upon the earth : and it was so . 1 : 16 And God made two great lights ; the greater light to rule the day , and the lesser light to rule the night : he made the stars also . 1 : 17 And God set them in the firmament of the heaven "
     ]
    }
   ],
   "source": [
    "for word in bible[:500]:\n",
    "    print(word, sep=' ',end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "AI=\"\"\"Artificial intelligence (AI) is intelligence demonstrated by machines, unlike the natural intelligence displayed by humans\n",
    "and animals, which involves consciousness and emotionality. The distinction between the former and the latter categories is \n",
    "often revealed by the acronym chosen. 'Strong' AI is usually labelled as artificial general intelligence (AGI) while attempts\n",
    "to emulate 'natural' intelligence have been called artificial biological intelligence (ABI). Leading AI textbooks define the \n",
    "field as the study of \"intelligent agents\": any device that perceives its environment and takes actions that maximize its \n",
    "chance of successfully achieving its goals.[3] Colloquially, the term \"artificial intelligence\" is often used to describe \n",
    "machines that mimic \"cognitive\" functions that humans associate with the human mind, such as \"learning\" and \"problem solving\".\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(AI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "AI_tokens=word_tokenize(AI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Artificial',\n",
       " 'intelligence',\n",
       " '(',\n",
       " 'AI',\n",
       " ')',\n",
       " 'is',\n",
       " 'intelligence',\n",
       " 'demonstrated',\n",
       " 'by',\n",
       " 'machines',\n",
       " ',',\n",
       " 'unlike',\n",
       " 'the',\n",
       " 'natural',\n",
       " 'intelligence',\n",
       " 'displayed',\n",
       " 'by',\n",
       " 'humans',\n",
       " 'and',\n",
       " 'animals',\n",
       " ',',\n",
       " 'which',\n",
       " 'involves',\n",
       " 'consciousness',\n",
       " 'and',\n",
       " 'emotionality',\n",
       " '.',\n",
       " 'The',\n",
       " 'distinction',\n",
       " 'between',\n",
       " 'the',\n",
       " 'former',\n",
       " 'and',\n",
       " 'the',\n",
       " 'latter',\n",
       " 'categories',\n",
       " 'is',\n",
       " 'often',\n",
       " 'revealed',\n",
       " 'by',\n",
       " 'the',\n",
       " 'acronym',\n",
       " 'chosen',\n",
       " '.',\n",
       " \"'Strong\",\n",
       " \"'\",\n",
       " 'AI',\n",
       " 'is',\n",
       " 'usually',\n",
       " 'labelled',\n",
       " 'as',\n",
       " 'artificial',\n",
       " 'general',\n",
       " 'intelligence',\n",
       " '(',\n",
       " 'AGI',\n",
       " ')',\n",
       " 'while',\n",
       " 'attempts',\n",
       " 'to',\n",
       " 'emulate',\n",
       " \"'natural\",\n",
       " \"'\",\n",
       " 'intelligence',\n",
       " 'have',\n",
       " 'been',\n",
       " 'called',\n",
       " 'artificial',\n",
       " 'biological',\n",
       " 'intelligence',\n",
       " '(',\n",
       " 'ABI',\n",
       " ')',\n",
       " '.',\n",
       " 'Leading',\n",
       " 'AI',\n",
       " 'textbooks',\n",
       " 'define',\n",
       " 'the',\n",
       " 'field',\n",
       " 'as',\n",
       " 'the',\n",
       " 'study',\n",
       " 'of',\n",
       " '``',\n",
       " 'intelligent',\n",
       " 'agents',\n",
       " \"''\",\n",
       " ':',\n",
       " 'any',\n",
       " 'device',\n",
       " 'that',\n",
       " 'perceives',\n",
       " 'its',\n",
       " 'environment',\n",
       " 'and',\n",
       " 'takes',\n",
       " 'actions',\n",
       " 'that',\n",
       " 'maximize',\n",
       " 'its',\n",
       " 'chance',\n",
       " 'of',\n",
       " 'successfully',\n",
       " 'achieving',\n",
       " 'its',\n",
       " 'goals',\n",
       " '.',\n",
       " '[',\n",
       " '3',\n",
       " ']',\n",
       " 'Colloquially',\n",
       " ',',\n",
       " 'the',\n",
       " 'term',\n",
       " '``',\n",
       " 'artificial',\n",
       " 'intelligence',\n",
       " \"''\",\n",
       " 'is',\n",
       " 'often',\n",
       " 'used',\n",
       " 'to',\n",
       " 'describe',\n",
       " 'machines',\n",
       " 'that',\n",
       " 'mimic',\n",
       " '``',\n",
       " 'cognitive',\n",
       " \"''\",\n",
       " 'functions',\n",
       " 'that',\n",
       " 'humans',\n",
       " 'associate',\n",
       " 'with',\n",
       " 'the',\n",
       " 'human',\n",
       " 'mind',\n",
       " ',',\n",
       " 'such',\n",
       " 'as',\n",
       " '``',\n",
       " 'learning',\n",
       " \"''\",\n",
       " 'and',\n",
       " '``',\n",
       " 'problem',\n",
       " 'solving',\n",
       " \"''\",\n",
       " '.']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AI_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for finding the frequency of words\n",
    "from nltk.probability import FreqDist\n",
    "fdist=FreqDist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'the': 9, 'intelligence': 7, 'and': 5, '.': 5, '``': 5, \"''\": 5, 'artificial': 4, 'is': 4, ',': 4, 'that': 4, ...})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for word in AI_tokens:\n",
    "     fdist[word.lower()]+=1\n",
    "fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 9),\n",
       " ('intelligence', 7),\n",
       " ('and', 5),\n",
       " ('.', 5),\n",
       " ('``', 5),\n",
       " (\"''\", 5),\n",
       " ('artificial', 4),\n",
       " ('is', 4),\n",
       " (',', 4),\n",
       " ('that', 4)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist_top10=fdist.most_common(10)\n",
    "fdist_top10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import blankline_tokenize\n",
    "AI_blank=blankline_tokenize(AI)\n",
    "len(AI_blank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "AI_sent=sent_tokenize(AI)\n",
    "len(AI_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import bigrams, trigrams, ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'best',\n",
       " 'and',\n",
       " 'most',\n",
       " 'beatiful',\n",
       " 'things',\n",
       " 'in',\n",
       " 'the',\n",
       " 'world',\n",
       " 'can',\n",
       " 'not',\n",
       " 'be',\n",
       " 'seen',\n",
       " 'or',\n",
       " 'even',\n",
       " 'touched',\n",
       " ',',\n",
       " 'they',\n",
       " 'must',\n",
       " 'be',\n",
       " 'felt',\n",
       " 'with',\n",
       " 'the',\n",
       " 'heart']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string=\"The best and most beatiful things in the world cannot be seen  or even touched ,they must be felt with the heart\"\n",
    "quotes_tokens=nltk.word_tokenize(string)\n",
    "quotes_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'best'),\n",
       " ('best', 'and'),\n",
       " ('and', 'most'),\n",
       " ('most', 'beatiful'),\n",
       " ('beatiful', 'things'),\n",
       " ('things', 'in'),\n",
       " ('in', 'the'),\n",
       " ('the', 'world'),\n",
       " ('world', 'can'),\n",
       " ('can', 'not'),\n",
       " ('not', 'be'),\n",
       " ('be', 'seen'),\n",
       " ('seen', 'or'),\n",
       " ('or', 'even'),\n",
       " ('even', 'touched'),\n",
       " ('touched', ','),\n",
       " (',', 'they'),\n",
       " ('they', 'must'),\n",
       " ('must', 'be'),\n",
       " ('be', 'felt'),\n",
       " ('felt', 'with'),\n",
       " ('with', 'the'),\n",
       " ('the', 'heart')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quotes_bigrams=list(nltk.bigrams(quotes_tokens))\n",
    "quotes_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'best', 'and'),\n",
       " ('best', 'and', 'most'),\n",
       " ('and', 'most', 'beatiful'),\n",
       " ('most', 'beatiful', 'things'),\n",
       " ('beatiful', 'things', 'in'),\n",
       " ('things', 'in', 'the'),\n",
       " ('in', 'the', 'world'),\n",
       " ('the', 'world', 'can'),\n",
       " ('world', 'can', 'not'),\n",
       " ('can', 'not', 'be'),\n",
       " ('not', 'be', 'seen'),\n",
       " ('be', 'seen', 'or'),\n",
       " ('seen', 'or', 'even'),\n",
       " ('or', 'even', 'touched'),\n",
       " ('even', 'touched', ','),\n",
       " ('touched', ',', 'they'),\n",
       " (',', 'they', 'must'),\n",
       " ('they', 'must', 'be'),\n",
       " ('must', 'be', 'felt'),\n",
       " ('be', 'felt', 'with'),\n",
       " ('felt', 'with', 'the'),\n",
       " ('with', 'the', 'heart')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quotes_trigrams=list(nltk.trigrams(quotes_tokens))\n",
    "quotes_trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'best', 'and', 'most'),\n",
       " ('best', 'and', 'most', 'beatiful'),\n",
       " ('and', 'most', 'beatiful', 'things'),\n",
       " ('most', 'beatiful', 'things', 'in'),\n",
       " ('beatiful', 'things', 'in', 'the'),\n",
       " ('things', 'in', 'the', 'world'),\n",
       " ('in', 'the', 'world', 'can'),\n",
       " ('the', 'world', 'can', 'not'),\n",
       " ('world', 'can', 'not', 'be'),\n",
       " ('can', 'not', 'be', 'seen'),\n",
       " ('not', 'be', 'seen', 'or'),\n",
       " ('be', 'seen', 'or', 'even'),\n",
       " ('seen', 'or', 'even', 'touched'),\n",
       " ('or', 'even', 'touched', ','),\n",
       " ('even', 'touched', ',', 'they'),\n",
       " ('touched', ',', 'they', 'must'),\n",
       " (',', 'they', 'must', 'be'),\n",
       " ('they', 'must', 'be', 'felt'),\n",
       " ('must', 'be', 'felt', 'with'),\n",
       " ('be', 'felt', 'with', 'the'),\n",
       " ('felt', 'with', 'the', 'heart')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quotes_ngrams=list(nltk.ngrams(quotes_tokens,4))\n",
    "quotes_ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'best', 'and', 'most', 'beatiful'),\n",
       " ('best', 'and', 'most', 'beatiful', 'things'),\n",
       " ('and', 'most', 'beatiful', 'things', 'in'),\n",
       " ('most', 'beatiful', 'things', 'in', 'the'),\n",
       " ('beatiful', 'things', 'in', 'the', 'world'),\n",
       " ('things', 'in', 'the', 'world', 'can'),\n",
       " ('in', 'the', 'world', 'can', 'not'),\n",
       " ('the', 'world', 'can', 'not', 'be'),\n",
       " ('world', 'can', 'not', 'be', 'seen'),\n",
       " ('can', 'not', 'be', 'seen', 'or'),\n",
       " ('not', 'be', 'seen', 'or', 'even'),\n",
       " ('be', 'seen', 'or', 'even', 'touched'),\n",
       " ('seen', 'or', 'even', 'touched', ','),\n",
       " ('or', 'even', 'touched', ',', 'they'),\n",
       " ('even', 'touched', ',', 'they', 'must'),\n",
       " ('touched', ',', 'they', 'must', 'be'),\n",
       " (',', 'they', 'must', 'be', 'felt'),\n",
       " ('they', 'must', 'be', 'felt', 'with'),\n",
       " ('must', 'be', 'felt', 'with', 'the'),\n",
       " ('be', 'felt', 'with', 'the', 'heart')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quotes_ngrams=list(nltk.ngrams(quotes_tokens,5))\n",
    "quotes_ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "pst=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'have'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pst.stem(\"having\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'give'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pst.stem(\"giving\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'happi'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pst.stem(\"happiness\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "give:give\n",
      "giving:give\n",
      "given:given\n",
      "gave:gave\n"
     ]
    }
   ],
   "source": [
    "words_stem=[\"give\",\"giving\",\"given\",\"gave\"]\n",
    "for words in words_stem:\n",
    "    print(words+\":\"+pst.stem(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "lst=LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "give:giv\n",
      "giving:giv\n",
      "given:giv\n",
      "gave:gav\n"
     ]
    }
   ],
   "source": [
    "for words in words_stem:\n",
    "    print(words+\":\"+lst.stem(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "word_lem=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\MURAHARI\n",
      "[nltk_data]     ASHOK\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "give:give\n",
      "giving:giving\n",
      "given:given\n",
      "gave:gave\n"
     ]
    }
   ],
   "source": [
    "for words in words_stem:\n",
    "       print(words+\":\"+word_lem.lemmatize(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\MURAHARI\n",
      "[nltk_data]     ASHOK\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 9),\n",
       " ('intelligence', 7),\n",
       " ('and', 5),\n",
       " ('.', 5),\n",
       " ('``', 5),\n",
       " (\"''\", 5),\n",
       " ('artificial', 4),\n",
       " ('is', 4),\n",
       " (',', 4),\n",
       " ('that', 4)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist_top10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "punctuation=re.compile(r'[-.?!:;,()|0-9]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_punctuation=[]\n",
    "for words in AI_tokens:\n",
    "    word=punctuation.sub(\"\",words)\n",
    "    if len(word)>0:\n",
    "        post_punctuation.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Artificial',\n",
       " 'intelligence',\n",
       " 'AI',\n",
       " 'is',\n",
       " 'intelligence',\n",
       " 'demonstrated',\n",
       " 'by',\n",
       " 'machines',\n",
       " 'unlike',\n",
       " 'the',\n",
       " 'natural',\n",
       " 'intelligence',\n",
       " 'displayed',\n",
       " 'by',\n",
       " 'humans',\n",
       " 'and',\n",
       " 'animals',\n",
       " 'which',\n",
       " 'involves',\n",
       " 'consciousness',\n",
       " 'and',\n",
       " 'emotionality',\n",
       " 'The',\n",
       " 'distinction',\n",
       " 'between',\n",
       " 'the',\n",
       " 'former',\n",
       " 'and',\n",
       " 'the',\n",
       " 'latter',\n",
       " 'categories',\n",
       " 'is',\n",
       " 'often',\n",
       " 'revealed',\n",
       " 'by',\n",
       " 'the',\n",
       " 'acronym',\n",
       " 'chosen',\n",
       " \"'Strong\",\n",
       " \"'\",\n",
       " 'AI',\n",
       " 'is',\n",
       " 'usually',\n",
       " 'labelled',\n",
       " 'as',\n",
       " 'artificial',\n",
       " 'general',\n",
       " 'intelligence',\n",
       " 'AGI',\n",
       " 'while',\n",
       " 'attempts',\n",
       " 'to',\n",
       " 'emulate',\n",
       " \"'natural\",\n",
       " \"'\",\n",
       " 'intelligence',\n",
       " 'have',\n",
       " 'been',\n",
       " 'called',\n",
       " 'artificial',\n",
       " 'biological',\n",
       " 'intelligence',\n",
       " 'ABI',\n",
       " 'Leading',\n",
       " 'AI',\n",
       " 'textbooks',\n",
       " 'define',\n",
       " 'the',\n",
       " 'field',\n",
       " 'as',\n",
       " 'the',\n",
       " 'study',\n",
       " 'of',\n",
       " '``',\n",
       " 'intelligent',\n",
       " 'agents',\n",
       " \"''\",\n",
       " 'any',\n",
       " 'device',\n",
       " 'that',\n",
       " 'perceives',\n",
       " 'its',\n",
       " 'environment',\n",
       " 'and',\n",
       " 'takes',\n",
       " 'actions',\n",
       " 'that',\n",
       " 'maximize',\n",
       " 'its',\n",
       " 'chance',\n",
       " 'of',\n",
       " 'successfully',\n",
       " 'achieving',\n",
       " 'its',\n",
       " 'goals',\n",
       " '[',\n",
       " ']',\n",
       " 'Colloquially',\n",
       " 'the',\n",
       " 'term',\n",
       " '``',\n",
       " 'artificial',\n",
       " 'intelligence',\n",
       " \"''\",\n",
       " 'is',\n",
       " 'often',\n",
       " 'used',\n",
       " 'to',\n",
       " 'describe',\n",
       " 'machines',\n",
       " 'that',\n",
       " 'mimic',\n",
       " '``',\n",
       " 'cognitive',\n",
       " \"''\",\n",
       " 'functions',\n",
       " 'that',\n",
       " 'humans',\n",
       " 'associate',\n",
       " 'with',\n",
       " 'the',\n",
       " 'human',\n",
       " 'mind',\n",
       " 'such',\n",
       " 'as',\n",
       " '``',\n",
       " 'learning',\n",
       " \"''\",\n",
       " 'and',\n",
       " '``',\n",
       " 'problem',\n",
       " 'solving',\n",
       " \"''\"]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "133"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(post_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(AI_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sent='Ashok is a good boy'\n",
    "sent_tokens=word_tokenize(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent='Ashok is a good boy'\n",
    "sent_tokens=word_tokenize(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\MURAHARI ASHOK\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Artificial', 'JJ')]\n",
      "[('intelligence', 'NN')]\n",
      "[('(', '(')]\n",
      "[('AI', 'NN')]\n",
      "[(')', ')')]\n",
      "[('is', 'VBZ')]\n",
      "[('intelligence', 'NN')]\n",
      "[('demonstrated', 'VBN')]\n",
      "[('by', 'IN')]\n",
      "[('machines', 'NNS')]\n",
      "[(',', ',')]\n",
      "[('unlike', 'IN')]\n",
      "[('the', 'DT')]\n",
      "[('natural', 'JJ')]\n",
      "[('intelligence', 'NN')]\n",
      "[('displayed', 'NN')]\n",
      "[('by', 'IN')]\n",
      "[('humans', 'NNS')]\n",
      "[('and', 'CC')]\n",
      "[('animals', 'NNS')]\n",
      "[(',', ',')]\n",
      "[('which', 'WDT')]\n",
      "[('involves', 'NNS')]\n",
      "[('consciousness', 'NN')]\n",
      "[('and', 'CC')]\n",
      "[('emotionality', 'NN')]\n",
      "[('.', '.')]\n",
      "[('The', 'DT')]\n",
      "[('distinction', 'NN')]\n",
      "[('between', 'IN')]\n",
      "[('the', 'DT')]\n",
      "[('former', 'JJ')]\n",
      "[('and', 'CC')]\n",
      "[('the', 'DT')]\n",
      "[('latter', 'NN')]\n",
      "[('categories', 'NNS')]\n",
      "[('is', 'VBZ')]\n",
      "[('often', 'RB')]\n",
      "[('revealed', 'VBD')]\n",
      "[('by', 'IN')]\n",
      "[('the', 'DT')]\n",
      "[('acronym', 'NN')]\n",
      "[('chosen', 'NN')]\n",
      "[('.', '.')]\n",
      "[(\"'Strong\", 'NNS')]\n",
      "[(\"'\", \"''\")]\n",
      "[('AI', 'NN')]\n",
      "[('is', 'VBZ')]\n",
      "[('usually', 'RB')]\n",
      "[('labelled', 'VBN')]\n",
      "[('as', 'IN')]\n",
      "[('artificial', 'JJ')]\n",
      "[('general', 'JJ')]\n",
      "[('intelligence', 'NN')]\n",
      "[('(', '(')]\n",
      "[('AGI', 'NN')]\n",
      "[(')', ')')]\n",
      "[('while', 'IN')]\n",
      "[('attempts', 'NNS')]\n",
      "[('to', 'TO')]\n",
      "[('emulate', 'NN')]\n",
      "[(\"'natural\", 'JJ')]\n",
      "[(\"'\", \"''\")]\n",
      "[('intelligence', 'NN')]\n",
      "[('have', 'VB')]\n",
      "[('been', 'VBN')]\n",
      "[('called', 'VBN')]\n",
      "[('artificial', 'JJ')]\n",
      "[('biological', 'JJ')]\n",
      "[('intelligence', 'NN')]\n",
      "[('(', '(')]\n",
      "[('ABI', 'NN')]\n",
      "[(')', ')')]\n",
      "[('.', '.')]\n",
      "[('Leading', 'VBG')]\n",
      "[('AI', 'NN')]\n",
      "[('textbooks', 'NNS')]\n",
      "[('define', 'NN')]\n",
      "[('the', 'DT')]\n",
      "[('field', 'NN')]\n",
      "[('as', 'IN')]\n",
      "[('the', 'DT')]\n",
      "[('study', 'NN')]\n",
      "[('of', 'IN')]\n",
      "[('``', '``')]\n",
      "[('intelligent', 'NN')]\n",
      "[('agents', 'NNS')]\n",
      "[(\"''\", \"''\")]\n",
      "[(':', ':')]\n",
      "[('any', 'DT')]\n",
      "[('device', 'NN')]\n",
      "[('that', 'IN')]\n",
      "[('perceives', 'NNS')]\n",
      "[('its', 'PRP$')]\n",
      "[('environment', 'NN')]\n",
      "[('and', 'CC')]\n",
      "[('takes', 'VBZ')]\n",
      "[('actions', 'NNS')]\n",
      "[('that', 'IN')]\n",
      "[('maximize', 'VB')]\n",
      "[('its', 'PRP$')]\n",
      "[('chance', 'NN')]\n",
      "[('of', 'IN')]\n",
      "[('successfully', 'RB')]\n",
      "[('achieving', 'VBG')]\n",
      "[('its', 'PRP$')]\n",
      "[('goals', 'NNS')]\n",
      "[('.', '.')]\n",
      "[('[', 'NN')]\n",
      "[('3', 'CD')]\n",
      "[(']', 'NN')]\n",
      "[('Colloquially', 'RB')]\n",
      "[(',', ',')]\n",
      "[('the', 'DT')]\n",
      "[('term', 'NN')]\n",
      "[('``', '``')]\n",
      "[('artificial', 'JJ')]\n",
      "[('intelligence', 'NN')]\n",
      "[(\"''\", \"''\")]\n",
      "[('is', 'VBZ')]\n",
      "[('often', 'RB')]\n",
      "[('used', 'VBN')]\n",
      "[('to', 'TO')]\n",
      "[('describe', 'NN')]\n",
      "[('machines', 'NNS')]\n",
      "[('that', 'IN')]\n",
      "[('mimic', 'NN')]\n",
      "[('``', '``')]\n",
      "[('cognitive', 'NN')]\n",
      "[(\"''\", \"''\")]\n",
      "[('functions', 'NNS')]\n",
      "[('that', 'IN')]\n",
      "[('humans', 'NNS')]\n",
      "[('associate', 'NN')]\n",
      "[('with', 'IN')]\n",
      "[('the', 'DT')]\n",
      "[('human', 'NN')]\n",
      "[('mind', 'NN')]\n",
      "[(',', ',')]\n",
      "[('such', 'JJ')]\n",
      "[('as', 'IN')]\n",
      "[('``', '``')]\n",
      "[('learning', 'VBG')]\n",
      "[(\"''\", \"''\")]\n",
      "[('and', 'CC')]\n",
      "[('``', '``')]\n",
      "[('problem', 'NN')]\n",
      "[('solving', 'VBG')]\n",
      "[(\"''\", \"''\")]\n",
      "[('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "for token in AI_tokens:\n",
    "    print(nltk.pos_tag([token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ne_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "NE_sent=\"Microsoft CEO Sunder pichai from IIT kharaghpur\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "NE_tokens=nltk.word_tokenize(NE_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "NE_tags=nltk.pos_tag(NE_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to C:\\Users\\MURAHARI\n",
      "[nltk_data]     ASHOK\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('maxent_ne_chunker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to C:\\Users\\MURAHARI\n",
      "[nltk_data]     ASHOK\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON Microsoft/NNP)\n",
      "  (ORGANIZATION CEO/NNP)\n",
      "  Sunder/NNP\n",
      "  pichai/NN\n",
      "  from/IN\n",
      "  (ORGANIZATION IIT/NNP)\n",
      "  kharaghpur/VB)\n"
     ]
    }
   ],
   "source": [
    "NE_NER=ne_chunk(NE_tags)\n",
    "print(NE_NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DT'),\n",
       " ('big', 'JJ'),\n",
       " ('cat', 'NN'),\n",
       " ('ate', 'VBD'),\n",
       " ('the', 'DT'),\n",
       " ('little', 'JJ'),\n",
       " ('mouse', 'NN'),\n",
       " ('who', 'WP'),\n",
       " ('was', 'VBD'),\n",
       " ('after', 'IN'),\n",
       " ('fresh', 'JJ'),\n",
       " ('cheese', 'NN')]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new=\"The big cat ate the little mouse who was after fresh cheese\"\n",
    "new_tokens=nltk.pos_tag(word_tokenize(new))\n",
    "new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar_np=r\"NP:{<DT>?<JJ>*<NN>}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_parser=nltk.RegexpParser(grammar_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\MURAHA~1\\\\AppData\\\\Local\\\\Temp\\\\tmpnjtq0h0l.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\python\\lib\\site-packages\\IPython\\core\\formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    343\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 345\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    346\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\python\\lib\\site-packages\\nltk\\tree.py\u001b[0m in \u001b[0;36m_repr_png_\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    817\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m             \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    820\u001b[0m                 \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m             \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0min_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\MURAHA~1\\\\AppData\\\\Local\\\\Temp\\\\tmpnjtq0h0l.png'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Tree('S', [Tree('NP', [('The', 'DT'), ('big', 'JJ'), ('cat', 'NN')]), ('ate', 'VBD'), Tree('NP', [('the', 'DT'), ('little', 'JJ'), ('mouse', 'NN')]), ('who', 'WP'), ('was', 'VBD'), ('after', 'IN'), Tree('NP', [('fresh', 'JJ'), ('cheese', 'NN')])])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_result=chunk_parser.parse(new_tokens)\n",
    "chunk_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
